{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vidur Ground Truth Benchmark (T4 Colab Edition)\n",
    "\n",
    "This notebook implements the \"Ground Truth\" calibration experiment from the Vidur project, adapted for a standard Google Colab T4 environment.\n",
    "\n",
    "**Objective:** benchmark a small model (`TinyLlama/TinyLlama-1.1B-Chat-v1.0`) on a single T4 GPU to generate baseline metrics (TTFT, TPOT) for simulator calibration.\n",
    "\n",
    "### Adaptation for T4:\n",
    "- **Model:** `TinyLlama/TinyLlama-1.1B-Chat-v1.0` (Fits easily in 16GB VRAM)\n",
    "- **Parallelism:** TP=1, PP=1\n",
    "- **Backend:** vLLM (OpenAI-compatible server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vllm pandas numpy matplotlib requests nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Any\n",
    "import threading\n",
    "\n",
    "# Constants\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "GPU_MEMORY_UTILIZATION = 0.9\n",
    "PORT = 8000\n",
    "BASE_URL = f\"http://localhost:{PORT}/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Start vLLM Server\n",
    "We start the server in a background subprocess. This mimics a real deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_vllm_server(model_name: str):\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--model\", model_name,\n",
    "        \"--dtype\", \"half\",\n",
    "        \"--gpu-memory-utilization\", str(GPU_MEMORY_UTILIZATION),\n",
    "        \"--port\", str(PORT),\n",
    "        \"--trust-remote-code\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"Starting vLLM server: {'. '.join(cmd)}\")\n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    return process\n",
    "\n",
    "def wait_for_server(url: str, timeout: int = 600):\n",
    "    start = time.time()\n",
    "    while time.time() - start < timeout:\n",
    "        try:\n",
    "            response = requests.get(f\"{url}/models\")\n",
    "            if response.status_code == 200:\n",
    "                print(\"Server is ready!\")\n",
    "                return True\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            pass\n",
    "        time.sleep(5)\n",
    "        print(\"Waiting for server...\", end=\"\\r\")\n",
    "    print(\"Server failed to start.\")\n",
    "    return False\n",
    "\n",
    "# Start the server\n",
    "server_process = start_vllm_server(MODEL_NAME)\n",
    "if not wait_for_server(BASE_URL):\n",
    "    server_process.kill()\n",
    "    raise RuntimeError(\"Could not start vLLM server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Client & Metrics Logic\n",
    "We define a client that sends requests and measures:\n",
    "- **TTFT (Time to First Token):** Latency to receive the first stream chunk.\n",
    "- **TPOT (Time per Output Token):** Inter-token latency for decode.\n",
    "- **E2E:** Total request time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_request(prompt_len: int, decode_len: int) -> Dict[str, Any]:\n",
    "    # Approximate tokens with characters (1 token ~= 4 chars)\n",
    "    prompt_text = \"A\" * (prompt_len * 4)\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt_text,\n",
    "        \"max_tokens\": decode_len,\n",
    "        \"temperature\": 0.0,\n",
    "        \"stream\": True,\n",
    "        \"ignore_eos\": True\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    first_token_time = None\n",
    "    end_time = None\n",
    "    token_count = 0\n",
    "\n",
    "    try:\n",
    "        response = requests.post(f\"{BASE_URL}/completions\", json=payload, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        for chunk in response.iter_lines():\n",
    "            if chunk:\n",
    "                if chunk.startswith(b\"data: [DONE]\"):\n",
    "                    break\n",
    "                \n",
    "                # Timestamp immediately on receipt\n",
    "                now = time.time()\n",
    "                if first_token_time is None:\n",
    "                    first_token_time = now\n",
    "                \n",
    "                token_count += 1\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        ttft = (first_token_time - start_time) * 1000 if first_token_time else 0 # ms\n",
    "        total_latency = (end_time - start_time) # seconds\n",
    "        # TPOT = (Total - TTFT) / (N-1)\n",
    "        decode_time = (end_time - first_token_time) if first_token_time else 0\n",
    "        tpot = (decode_time * 1000) / (token_count - 1) if token_count > 1 else 0 # ms\n",
    "\n",
    "        return {\n",
    "            \"prompt_len\": prompt_len,\n",
    "            \"decode_len\": decode_len,\n",
    "            \"ttft_ms\": ttft,\n",
    "            \"tpot_ms\": tpot,\n",
    "            \"e2e_latency_s\": total_latency,\n",
    "            \"output_tokens\": token_count,\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phase A: Synthetic Probes\n",
    "Testing specific Prompt/Decode length combinations to isolate prefill vs decode performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probes = [\n",
    "    (128, 128),   # Short/Short\n",
    "    (512, 128),   # Medium/Short\n",
    "    (1024, 512),  # Long/Medium\n",
    "    (2048, 128)   # Heavy Prefill\n",
    "]
",
    "\n",
    "print(\"Running Phase A: Synthetic Probes...\")\n",
    "phase_a_results = []\n",
    "\n",
    "for p_len, d_len in probes:\n",
    "    print(f\"  Probing [{p_len}, {d_len}]...\")\n",
    "    # Warmup\n",
    "    run_request(p_len, 10)\n",
    "    \n",
    "    # Measurement (Avg of 3)\n",
    "    for _ in range(3):\n",
    "        res = run_request(p_len, d_len)\n",
    "        if res['status'] == 'success':\n",
    "            phase_a_results.append(res)\n",
    "            \n",
    "phase_a_df = pd.DataFrame(phase_a_results)\n",
    "print(\"\\nPhase A Results:\")\n",
    "print(phase_a_df.groupby(['prompt_len', 'decode_len'])[['ttft_ms', 'tpot_ms']].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Phase B: Saturation Sweep (Poisson)\n",
    "We generate requests arriving according to a Poisson process to test scheduler overhead and find the saturation point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_poisson_experiment(rps: float, duration_s: int = 30):\n",
    "    print(f\"\\nStarting Poisson Load Test: {rps} RPS for {duration_s}s\")\n",
    "    \n",
    "    # Generate arrival times\n",
    "    num_requests = int(rps * duration_s)\n",
    "    intervals = np.random.exponential(1.0/rps, num_requests)\n",
    "    arrival_times = np.cumsum(intervals)\n",
    "    \n",
    "    results = []\n",
    "    threads = []\n",
    "    \n",
    "    start_exp_time = time.time()\n",
    "    \n",
    "    def worker(delay):\n",
    "        time.sleep(delay)\n",
    "        # Use a fixed workload for stability: 512 prefill, 128 decode\n",
    "        res = run_request(512, 128)\n",
    "        results.append(res)\n",
    "\n",
    "    for delay in arrival_times:\n",
    "        t = threading.Thread(target=worker, args=(delay,))
",
    "        threads.append(t)\n",
    "        t.start()\n",
    "        \n",
    "    # Wait for all threads\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "        \n",
    "    return pd.DataFrame([r for r in results if r['status'] == 'success'])\n",
    "\n",
    "# Sweep RPS\n",
    "rps_levels = [0.5, 1.0, 2.0, 4.0] # Adjusted for T4 capability\n",
    "phase_b_metrics = {}\n",
    "\n",
    "for rps in rps_levels:\n",
    "    df = run_poisson_experiment(rps)\n",
    "    phase_b_metrics[rps] = {\n",
    "        \"ttft_p50\": df['ttft_ms'].median(),\n",
    "        \"ttft_p99\": df['ttft_ms'].quantile(0.99),\n",
    "        \"tpot_p50\": df['tpot_ms'].median(),\n",
    "        \"e2e_p99\": df['e2e_latency_s'].quantile(0.99)\n",
    "    }\n",
    "    print(f\"  -> P99 E2E Latency: {phase_b_metrics[rps]['e2e_p99']:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export Results\n",
    "We save the results in the JSON format specified in `EXPERIMENT_PLAN.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct JSON Output\n",
    "output_data = {\n",
    "    \"experiment_id\": \"tinyllama-1b-t4-colab-run1\",\n",
    "    \"config\": {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"tp\": 1,\n",
    "        \"pp\": 1,\n",
    "        \"gpu\": \"T4-16GB\"\n",
    "    },\n",
    "    \"workload\": {\n",
    "        \"type\": \"synthetic_poisson_mixed\",\n",
    "        \"rps_levels\": rps_levels,\n",
    "        \"phase_a_probes\": probes\n",
    "    },\n",
    "    \"metrics\": {\n",
    "        \"phase_a_raw\": phase_a_results,\n",
    "        \"phase_b_summary\": phase_b_metrics\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"benchmark_results.json\", \"w\") as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(\"Benchmark complete! Results saved to 'benchmark_results.json'.\")\n",
    "\n",
    "# Stop Server\n",
    "server_process.kill()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
